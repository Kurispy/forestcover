% 
% file: projectWriteup.tex 
% author: Ian Woods
%
 
\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{indentfirst}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in}

% Sample macros -- how you define new commands
% My own set of frequently-used macros have grown to many hundreds of lines.
% Here are some simple samples.
\newcommand{\Adv}{{\mathbf{Adv}}}       
\newcommand{\prp}{{\mathrm{prp}}}                  % How to define new commands 
\newcommand{\calK}{{\cal K}}
\newcommand{\outputs}{{\Rightarrow}}                
\newcommand{\getsr}{{\:\stackrel{{\scriptscriptstyle\hspace{0.2em}\$}}{\leftarrow}\:}}
\newcommand{\andthen}{{\::\;\;}}    % \, \: \; for thinspace, medspace, thickspace
\newcommand{\Rand}[1]{{\mathrm{Rand}[{#1}]}}       % A command with one argument
\newcommand{\Perm}[1]{{\mathrm{Perm}[{#1}]}}       
\newcommand{\Randd}[2]{{\mathrm{Rand}[{#1},{#2}]}} % and with two arguments


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\bf ECS 171 - Forest Cover Project Write-up\\[2ex] }
\date{\today}
\author{\bf Ian Woods, Jesse Dyer, Miguel Covarrubias}

\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Brief Description of KNN} 
The $k$-Nearest Neighbors (KNN) Algorithm involves classifying as a particular type, given the type of the $k$ closest samples to it, as determined by some distance function. Our initial approach was to use a standard Euclidean distance function, defined as follows:
\\
$\sqrt{\sum\limits_{k = 1}^n (x_{jk} - x_{ik})^2}$
\\
We decided to begin with this naive approach to isolate failure points (essentially, if something were to work incorrectly, we could rule out the distance function).
Later on, we intend to write (or better yet, generate with LMNN) a distance function specifically tailored to this dataset for nearly maximum correct classification.
\\
Using different values for $k$ can theoretically alter the predicted results - for example, if a sample of unknown type is closest to a sample of type $A$ with a calculated distance of 1, but two samples of type $B$ are at a distance of 1.1 from our unknown sample, choosing $k$ = 1 will yield the result $A$ for the unknown's type, and $k$ = 3 will yield the result $B$. We decided to test a wide range of $k$ values.
\section*{Results of Standard KNN Algorithm for Various K Values}
We ran 20 preliminary trial runs on the standard KNN algorithm and found that, at these low values, the predictor is relatively stable as $k$ changes. 

\begin{center}
  \begin{tabular}{ l | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | r }
    \hline
    $k$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline
    \% correct & 0.909845 & 0.908847 & 0.910052 & 0.908158 & 0.908503 & 0.910465 & 0.915628 & 0.910981 & 0.913150 \\
    \hline
  \end{tabular}
\end{center}

\begin{flushleft}
  \begin{tabular}{ l | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | r }
    \hline
    $k$ & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 \\ \hline
    \% correct & 0.913150 &  0.909122 & 0.908124 & 0.907504 & 0.906816 & 0.910843 & 0.911979 &  0.906506 & 0.909673 \\
    \hline
  \end{tabular}
\end{flushleft}

\begin{center}
  \begin{tabular}{ l | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | r }
    \hline
    $k$ & 19 & 20 \\ \hline
    \% correct  & 0.907229 & .906988 \\
    \hline
  \end{tabular}
\end{center}
Note to whoever is reading this before it's submitted - We will test higher $k$ values against the entire dataset (these were tested against 1/10th of the dataset to save time). Additionally, we will put up some pretty graphs, etc.
\section{Brief Description of LMNN}
Hey.
\section{Results of LMNN}
You.
\end{document}
